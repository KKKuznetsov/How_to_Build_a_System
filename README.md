Всем привет!
Меня зовут Кирилл.

Я проработал 7 лет в аналитической компании, которая реализовывала различные проекты для фармацевтических компаний в сфере обработки данных.
Конкретно мой отдел занимался обработкой информации о продажах дистрибьюторов и аптечных сетей, с которыми заключали контракты фармацевтические компании.

Одновременно в работе находилось более 40 проектов для клиентов, включая таких гигантов, как Teva, Novartis, AstraZeneca, Petrovax, Sandoz, Alcon и других не менее крупных компаний. Все они работали на российском рынке и получали регулярную отчётность от дистрибьюторов и аптечных сетей о продажах, закупках, остатках и других показателях — в формате ежедневных, еженедельных и ежемесячных данных.

Первоначально вся эта отчётность передавалась нам через подключение к нашему SFTP-серверу, где была настроена структура папок для хранения данных. Позже для этих целей был разработан многофункциональный портал для передачи данных на сервер.

# Организация хранилища исходных данных

Первое, с чего стоит начать работу в подобном проекте, — это организация структуры хранения исходных данных.
В качестве примера я покажу, как такая структура папок организована на моём компьютере.

Предположим, мы хотим разделить данные, которые планируем обрабатывать, на две сущности:

- Дистрибьюторы — данные о поставках в различные точки страны.
- Сети — данные о тех, кто может перепродавать полученный от дистрибьюторов товар.

Эти сущности можно объединить под общим названием "Поставщики данных".

Решим первую задачу.
Как сгенерировать структуру папок из 14 тысяч папок по заданным параметрам
Решим с помощью Python.
Определяем корневую структуру, в нашем случае это папка поставщики данных, в которой есть папки Сети и Дистрибьюторы.
В ручном режим задаем годы, которые будут в каждой из папок, я возьму 2023,2024,2025
Далее определяем параметры:
- В каждой папке года может быть 12 месяцев
- В каждой папке месяц по 40 клиентов
- В каждом клиенте по 5 типов закупок

Решение на Python.
[Массовое создание папок.py](https://github.com/KKKuznetsov/folder_generator_python)

После того как папки созданы, клиент может загружать туда данные.
Обычно список клиентов, типов закупок и прочих параметров структуры согласовывается заранее и настраивается на основе справочников от самого клиента.

# Учет данных.

Ведение учёта данных — это большой и важный блок, так как для этого создаются отдельные системы, которые позволяют не только хранить, но и отслеживать статус обработки файлов.
В моём опыте мы с коллегами разрабатывали подобную систему на базе 1С. В данном разделе я разберу ключевые моменты учёта данных на примере более универсального решения.

Представим ситуацию: мы ежедневно получаем новый набор файлов для обработки, при этом продолжаем хранить и учитывать уже ранее полученные и обработанные файлы.

Сразу зафиксируем следующие аксиомы:

1. Клиент не может удалять файлы с SFTP
2. Клиент не может изменять файлы на SFTP
3. Клиент может выложить файл только в конечную папку.

Решение задачи
Чтобы вести учёт данных, необходимо иметь полный список всех уже имеющихся файлов и где-то этот список хранить.
Для этого создадим хранилище на базе PostgreSQL, в которое будем записывать ключевые метрики для учёта и мониторинга:

- Путь к файлу;
- Дата, когда файл был выложен на SFTP;
- статус файла (новый, в обработке, обработан, ошибка).
- Поставщик данных (Сеть, Дистрибьютор)
- Год - (год, к которому относятся данные в отчете)
- Месяц (Месяц, к которому относятся данные в отчете)
- Клиент (Название клиента)
- Тип отчета (Закупки, продажи, остатки и другие типы)

Такое решение позволит автоматизировать процесс учёта и исключить дублирование, а также обеспечит прозрачность и контроль в любой момент времени.

Решение на PostgreSQL.
[https://github.com/KKKuznetsov/Create_table_file_registry_PostgreSQL

После того, как мы создали хранилище, мы можем наполнить его данными, для этого напишем скрипт на Python, который будет:

1. Проходить по всей структуре папок (SFTP-структуре или локальной копии).
2. Определять новые файлы (те, которых ещё нет в таблице).
3. Автоматически заполнять все ключевые поля таблицы.
   - file_path — из пути к файлу
   - uploaded_at — дата последней модификации файла (os.path.getmtime)
   - status — по умолчанию "NEW"
   - data_provider, report_year, report_month, client_name, report_type — можно разобрать из пути по шаблону
4. Записывать в PostgreSQL новые строки.

Решение на Python.
https://github.com/KKKuznetsov/Python_Scanner

Сканер протестирован и удачно добавляет данные в базу.
Но как нам теперь сделать так, чтобы скрипт запускался регулярно, скажем по заданному расписанию и автоматически добавлял новые записи в базу?
Решим задачу через планировщик заданий.

Планировщик задач.
https://github.com/KKKuznetsov/Python_TaskScheduler

Теперь наш сканер запускается каждые 15 минут и автоматически добавляет их в нашу систему учета на PostgreSQL.

# Автоматизация загрузки полученных от клиентов данных в исходные таблицы.
Т.к у нас есть уже настроенная система учета исходных данных, мы можем настроить автоматизированный процесс их загрузки в исходные таблицы.
Решим эту задачу с использованием PostgreSQL + программы на Python, которая будет обрабатывать файлы и сохранять их в нужной нам папке.

## Создание исходника в PostgreSQL, где будут храниться обработанные файлы.
В данном случае важно понимать, какие данные мы храним.
Очень часто бывает, что клиент сам не понимает, какие данные ему нужны, поэтому по возможности лучше загружать максимум данных из тех, что есть в исходных данных.
Представим, что в исходных данных мы получаем данные по следующим метрикам:

- Поставщик данных
- Id поставщика отчета (для внутреннего пользования)
- Филиал поставщика данных
- Период, за который предоставлен отчет
- Товар
- Клиенты (адреса, юрлица, инн, география)
- Канал сбыта
- Различные кодировки (код точки продаж, код скю)
- Количество товара
  
Спроектируем набор полей нашего исходника и создадим его в PostgreSQL.

Решение на PostgreSQL.
https://github.com/KKKuznetsov/Primary_data

## Подготовка исходных данных.
После того, как хранилище для исходных данных было создано, опишем как автоматизировать процесс подготовки исходных данных к загрузке.

Идея.
Каждые 15 минут оркестратор start_processing.py:
1. Берёт задачи из ops.file_registry со статусами NEW / PROCESSING / ERROR;
2. находит подходящий клиентский скрипт и запускает его только для конкретного id;
3. скрипт формирует отчёт и пишет его во временную папку «Итоговые отчёты» с именем, содержащим _id{ID}_;
4. оркестратор переносит готовые файлы в «Данные на загрузку» и обновляет статус записи до CREATED в базе данных; при проблемах — ERROR с пояснением в error_reason.

Статусы в БД: NEW, PROCESSING, CREATED, ERROR, DELETE.
Краткое пояснение по статусам
NEW - новый файл.
PROCESSING - файл скрипта обрабатывающий отчет отсутствует.
CREATED - файл создан.
ERROR - скрипт обрабатывающий файл есть, но завершился с ошибкой.
DELETE - файл удален.

Примеры error_reason: NO_SCRIPT_FOUND, NO_OUTPUT_FILE, TIMEOUT, LOCKED, NO_SPACE, PATH_TOO_LONG, RETURN_CODE_X.

Структура папок:

C:\Users\user\Desktop\Python_scripts\automated_processing\

├─ start_processing.py              # оркестратор

├─ Reestr\
│   └─ new_files_registry.csv       # реестр текущего запуска (read-only)

├─ Scripts\

│   ├─ Distibutors\                 # поставщики типа "Дистрибьютор"

│   │   └─ Client_01\Client_01_processing.py

    │   └─ Client_01\Client_02_processing.py
    
    │   └─ Client_01\Client_03_processing.py
   
├─ report_header\
│   └─ report_header.xlsx           # эталонная шапка (схема целевых колонок)

Рабочие каталоги для файлов:

C:\Users\user\Desktop\Итоговые отчеты — сюда клиентские скрипты сохраняют промежуточные результаты.
C:\Users\user\Desktop\Данные на загрузку — сюда оркестратор переносит проверенные файлы для дальнейшей загрузки.

Контракт клиентского скрипта:

Оркестратор запускает скрипт с переменными окружения
- TASK_ID — обязателен; обрабатывайте ровно эту запись;
- TASK_FILE, TASK_CLIENT, TASK_REPORT_TYPE — вспомогательные.

Скрипт:

- читает исходник (csv, xls, xlsx), приводит поля к схеме report_header.xlsx;
- сохраняет файл в «Итоговые отчёты» с именем вида: {Client}_id{ID}_{source_basename}_{YYYYMMDD_HHMMSS}.xlsx

Оркестратор: ключевые моменты:

- Единственный запуск через PostgreSQL advisory lock (pg_try_advisory_lock).
- Осторожная очистка «Итоговых отчётов» (по возрасту или карантин), чтобы не удалить «живые» файлы.
- Перенос с ретраями и защитой от коллизий имён (хеш/размер, суффикс времени).
- Статусы в БД обновляются по месту: успех → CREATED, неуспех → ERROR + error_reason.

Программа будет запускаться сразу после окончания выполнения скрипта Python_Scanner.

Решение на Python.
